{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central problem in machine learning is learning how to ensure that an algorithm will perform well out of sample. A core tradeoff in the machine learning literature is that between bias and variance: as a model becomes more accurate in-sample (as the bias decreases), its performance becomes increasingly volatile out of sample, leading to overfitting. One tool we have to improve model generalization is regularization. \n",
    "\n",
    "Regularization includes any strategy intended to reduce generalization error while leaving training error unaffected. In general, these methods include constraints placed on parameter values, modifications of cost functions, and the purposeful injection of noise into the training data. In general, when the data generating mechanism we seek to imitate is complex, the best approach is to fit a large model with a sufficient amount of regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Norm Penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization via parameter norm penalties has been used for several decades, having entered the literature long before the rise of deep learning. Parameter norm penalties place a penalty $ \\Omega(\\theta) $ to the objective function $J$. The new objectiv function, then, is:\n",
    "\n",
    "$$ \\tilde{J}(\\theta;X,y) = J(\\theta;X,y) + \\alpha \\Omega(\\theta), $$\n",
    "\n",
    "where $\\alpha \\in [0, \\infty) $  is a hyperparameter taht weighs the contribution of the norm penalty term $\\Omega$ relative to the standard objective function $J$. This added term penalizes the size of the model's weights, enforcing a tradeoff between the conflicting minimizations of the original cost function and the norm of the model weights. In neural networks, the penalty term typically only affects the weights, with the biases left unaffected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $L^2$ penalty is the simplest and most common type of norm penalty, known commonly as **weight decay**. This penalty drives weights closer to the origin by adding the regularization term $ \\Omega(\\theta) = \\frac{1}{2}||\\textbf w ||_{2}^2 $ to the objective function. This penalty may be familiar to those who have studied **ridge regression**. \n",
    "\n",
    "As $\\alpha$ from the objective function stated above approaches 0, the regularized model approaches the unregularized model. Therefore, the larger $\\alpha$ becomes, the more this regularization term pulls the model's weights toward zero. This keeps the model from fitting noise in the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L^1$ regularization is similar to $L^2$ regularization, but using a different norm. The penalty term is now defined as:\n",
    "\n",
    "$$ \\Omega(\\theta) = ||w||_{1} = \\sum_{i} |w_{i}|, $$\n",
    "\n",
    "the sum of the absolute values of the weights. Where $L^2$ regularization was analagous to ridge regression, $L^1$ regularization will be familiar to anyone who has worked with the **LASSO** model. In practice, the main difference between applying $L^1$  and $L^2$ regularization is that while $L^2$ regularization is a form of weight decay, causing weight values to be smaller, $L^1$ regularization tends to bring weights all the way down to zero, causing models to become **sparse**. This property is especially useful in high-dimensional data sets. This penalty is commonly used as a mechanism for **feature selection**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norm Penalties as Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One view of the affect of norm penalties on model training is that they introduce a budget on weight values. While increasing weight values helps the model to fit noise, forcing the sum of the model's weights to be below a set value, as is the case in $L^1$ regularization, restricts the model's freedom to overfit in this way. Weight values must then be allocated only where there is the strongest signal. This can lead to better out of sample performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to improve a model's generalization is to train it on more data. This is often not possible. One workaround for thi is to create fake data and add it to the training set. If the fake data is similar enough to that which comes from the true data generating mechanism, this can improve performance. \n",
    "\n",
    "This approach is particularly effective for object recogntion problems. Objects can be reflected, moved to different areas of the image, and different types of noise can be added to images. \n",
    "\n",
    "Additionally, adding random noise to input features can improve a neural network's generalization and noise robustness while also increasing the size of the training set. Noise injection can work similarly well when noise is added to the hidden units. The performance of these approaches is often problem dependent, however, so some trial and error is necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar regularization technique is adding small amounts of noise to the weights. This is used primarily in the context of recurrent models. This can be interpreted as a stochastic implementation of Bayesian inference over the weights, where the weights are viewed to be uncertain and representable by a probability distribution, and the injected noise is a way of reflecting uncertainty. \n",
    "\n",
    "This can also be viewed as a more traditional form of regularization, adding an additional term to the cost function. Adding noise to the weights not only encourages weights to stay small, as is the case with L2 regularization, but it also forces them toward more stable minima, where the gradient is still small if the weights were to be moved in either direction by the added noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Injecting Noise at the Output Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data with unreliable labeling, it is sometimes helpful to replace hard labeling with soft, probabilistic labeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When multiple outputs $y^i$ are to be predicted by the same inputs $X$, these tasks can be carried out with a common set of hidden layers. This sharing of tasks can serve to prevent overfitting by forcing the layers to learn the common, higher-level patterns that represent all or most $y^i$, instead of any one $y$ individually. \n",
    "\n",
    "These models can generally be split into two types of parameters:\n",
    "\n",
    "* Task-specific parameters, which only feed into individual output nodes. These are in the final layers of the network. \n",
    "* Generic parameters, shared across tasks. These are in the lower layers of the network. \n",
    "\n",
    "The core belief of multitask learning is that among the factors that explain the variations observed in the data associated with different tasks, some are shared across two or more tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model will overfit if given sufficient training time and data. A constantly decreasing training error is not, therefore, necessarily evidence of an improving model. It is the test-set error that we really care about. Interestingly enough, stopping training once out-of-sample error begins to increase can actually be viewed as a type of regularization. \n",
    "\n",
    "The method for this is simple. At the end of each training epoch we store the loss and model weights. If the test-set loss gets worse for a predetermined number of consecutive epochs, we revert back to the weights from before performance began to decline and stop training the model. \n",
    "\n",
    "There are two costs to this technique. First, it makes the assumption that the model will not get any better if training continues. Loss functions are nonconvex, so we can never know with certainty that this is the case. This uncertainty is why we use a \"patience\" buffer, allowing the model to get worse for a set number of epochs to make sure it is not going to begin to improve again. Second, we need to store a copy of the best parameters. This cost is usually negligible, but it does take up some memory, which is sometimes in short supply. \n",
    "\n",
    "Early stopping is often used in conjunction with other regularization techniques. It does not directly impact the loss function, so it is reasonable to use this alongside a parameter norm penalty. \n",
    "\n",
    "As early stopping requires witholding some of your data, the best approach is often to re-train the model at the end for the same number of training steps. This introduces some ambiguity, however, in that there is no way to evaluate this final model out of sample. Nonetheless, the extra data used for training tends to help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Early Stopping Acts as a Regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping acts as a regularizer by effectively limiting the optimization procedure to a relatively small section of the parameter space in the neighborhood of the initial parameters $\\theta_{0}$. In the case of a linear model with a quadratic error function and gradient descent, this is equivalent to $L^2$ regularization. \n",
    "\n",
    "To compare with classic $L^2$ regularization, for simplicity's stake, first assume the only parameters are linear weights $\\theta = w$. With optimal weights $w^{*},$ the cost function is now:\n",
    "\n",
    "$$\\hat{J} (\\theta) = J(w^{*}) + \\frac{1}{2}(w - w^{*})^{T} H (w-w^{*},  $$\n",
    "\n",
    "Where $H$ is the Hessian matrix of $J$ with respect to $w$, evaluated at $w^{*}.$ Given that $w^*$ is the minimum of *J(w)$, we know that H is positive semidefinite. Under a local Taylor series approximation, the gradient is given by \n",
    "\n",
    "$$ \\triangledown_w \\hat{J}(w) = H(w - w^*) $$\n",
    "\n",
    "After some simplification, we can show that the trajectories followed by parameter vectors during training are the same for $L^2$ regularization and early stopping. For early stopping:\n",
    "\n",
    "$$ Q^T w^{(\\tau)} = [I - (I - \\epsilon A)^{\\tau}]Q^T w^*, $$\n",
    "\n",
    "and for $L^2$ regularization:\n",
    "\n",
    "$$ Q^T \\tilde{w} = [I - (A + \\alpha I)^{-1}\\alpha]Q^T w^*, $$\n",
    "\n",
    "where $Q, A, and Q^{-1}$ come from the eigendecomposition of $H,$ $\\tau$ represents parameter updates, and $\\epsilon$ is chosen to guarantee that $ |1 - \\epsilon \\lambda | < 1. $\n",
    "\n",
    "Comparing these two functions, if $\\epsilon$ and $\\tau$ are chosen such that\n",
    "\n",
    "$$ (I - \\epsilon A)^{\\tau} = (A + \\alpha I)^{-1}\\alpha, $$\n",
    "\n",
    "then the two techniques can be seen as equivalent. Further, by taking logs and a series expansion, we can also conclude that the number of training iterations is inversely proportional to the $L^2$ retularization parameter. That is, the more we train, the less we are regularizing our model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While weight decay places a penalty on the norm of the model's weight vectors, a similar approach can be taken with its activation functions. Penalizing activation units can cause a model's activations to be sparse. \n",
    "\n",
    "The difference here is that we introduce sparsity by adding a norm penalty to the elements of the representation $h$ rather than the weights $\\theta$. The augmented loss function is then\n",
    "\n",
    "$$ J(\\theta;X,y) = J(\\theta X,y) + \\alpha \\Omega (h), $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ \\Omega(h) = ||h||_{1} = \\sum_{i} h^{i}. $$ \n",
    "\n",
    "There are alternatives to the $L^1$ penalty here, but that is the one most commonly used. The result of representational sparsity tends to be similar to the result of sparsity in the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Other Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, short for **bootstrap aggregating**, is a technique for reducing generalization error by combining several models. The idea is to train several models separately, and then to have all models vote separately on the best outcome. Bagging works by resampling the training data with replacement, and then training multiple models on these resampled training sets. This means that there will be some repeated observations in each training set, and also that each model will achieve a slightly different representation of the data. These model-averaging methods are referred to as **ensemble methods**. Deep models reach a wide enough variety of representations of the same data that these methods can produce significant performance improvements over individual models. \n",
    "\n",
    "The effect of bagging is to produce a lower-variance estimator with similar bias to an individual model. This means that the in-sample error is roughly the same as an individual model, but the out-of-sample performance is more stable, and often better. The drawback to this method, however, is obvious. Neural networks can be slow to train, so it is not always reasonable to train a large number of these models in order to achieve incremental improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a computationally inexpensive but powerful method for regularization. This method can be viewed as a practical approximation to traiing a bagged ensemble of models. Dropout trains the ensemble consisting of all subnetworks that can be formed by removing nonoutput units from an underlying base network. We can typicall remove a unit from a network by multiplying its output value by zero (it is more complicated for certain types of networks, but we'll ignore these.)\n",
    "\n",
    "To train with Dropout, we use a minibatch-based learning algorithm such as stochastic gradient descent and apply a different binary mask for each training iteration. The mask multiplies each unit by either 1 or 0, deciding whether to keep or mute-out a unit. It is common practice to keep 80% of input weights, and 50% of the hidden-unit weights at each iteration. With this mask appllied, we the forward and back-propogate as usual. \n",
    "\n",
    "This is not exactly the same as ensembling, since the models are not independent of each other in a Dropout model. Instead, a tiny fraction of the possible sub-networks are trained at each step. Each time a new iteration begins, a new sample of subnetworks is chosen to be trained, inheriting the existing features and weights from the previous iteration. \n",
    "\n",
    "To make a prediction, a bagged ensemble must average votes from all its component models. With dropout, each submodel is defined by its mask vector $\\mu$, which defines a probability distribution $P(y|x,\\mu)$. The mean over all masks is given by\n",
    "\n",
    "$$ \\sum_{\\mu} p(\\mu)p(y|x,\\mu), $$\n",
    "\n",
    "where $p(\\mu)$ is the probability distribution used to sample $\\mu$ during training. This is intractable, however, because this sum includes an exponential number of terms and the model does not permit any form of simplification. Instead, we can approximate this inference by averaging together the output of a sufficiently large number of masks.\n",
    "\n",
    "A somewhat easier way of approximating this is called the **weight scaling inference rule**. This rule essentially states that if you multiply each weight by its probability of being active during training, you can use the entire network for inference without having to sample from the set of individually trained models. This rule applies slightly differently to different types of hidden units, but in general it just means multiplying weights by 0.5. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
