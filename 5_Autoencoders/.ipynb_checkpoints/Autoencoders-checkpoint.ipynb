{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **autoencoder** is a neural network that is trained to produce a copy of its input. Internally, the autoencoder contains a hidden layer $h$ which serves as a code to represent the input. The autoencoder can be viewed as two functions: \n",
    "\n",
    "* an encoder $h = f(x),$ which encodes the input in the hidden neurons \n",
    "* a decoder $\\rho = g(h),$ which decodes the hidden layer to produce an output.\n",
    "\n",
    "If the autoencoder is successful, $g(f(x)) = x$ everywhere. This is not very useful, however. We instead place constraints on the autoencoder so that it will only learn the most useful properties of the input.\n",
    "\n",
    "Historically, these models have been used for dimensionality reduction and feature learning. More recently, they have also taken an important role in generative modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercomplete Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seldom the output of an autoencoder that we care about. Rather, we are interested in training the model so that $\\textbf h$ takes on useful properties. A common way of doing this is to make the autoencoder *undercomplete*. An undercomplete autoencoder is one that has less hidden-layer dimensions than it has input dimensions. This forces the model to choose only the aspects of the input which are most important for the task of recreating it. \n",
    "\n",
    "The learning process is the same as with a feedforward network. A loss function $L(x, g(f(x)))$ is defined, penalizing $g(f(x))$ for being dissimilar from $x$. \n",
    "\n",
    "When the decoder is linear and $L$ is the MSE, then the autoencoder spans the same subspace as PCA. Autoencoders with nonlinear encoder and decoder functions can learn a more powerful nonlinear generalization of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization allows us to train autoenoders with hidden layers of equal or greater dimensions than their input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse autoencoder is an autoencoder whose training criterion involves a sparsity penalty $\\Omega(\\textbf h)$ on the code layer $\\textbf h$ , in addition to the reconstruction error:\n",
    "\n",
    "$$ L(\\textbf x, g(f(\\textbf x))) + \\Omega(\\textbf h), $$\n",
    "\n",
    "where $g(\\textbf h)$ is the decoder output, and typically we have $\\textbf h = f(\\textbf x) $, the encoder output. These models are commonly used to learn features for another task, such as classification. \n",
    "\n",
    "One can view the sparse autoencoder framework as approximating maximum likelihood training of a generative model with latent variables. We have visible variables $\\textbf x$ and latent variables $\\textbf h$, with joint distribution $ p_{model}(\\textbf x, \\textbf h) = p_{model}(\\textbf h)p_{model}(\\textbf x | \\textbf h).$ $p_{model}(\\textbf h)$ is the model's prior distribution over the latent variables, representing the model's beliefs prior to seeing $\\textbf x$. The log likelihood can be decomposed as:\n",
    "\n",
    "$$ log p_{model}(\\textbf x) = log \\sum_{\\textbf h} p_{model}(\\textbf h, \\textbf x). $$\n",
    "\n",
    "We can think of the autoencoder as approximating this sum with a point estimate for just one highly likely value of $\\textbf h $. With this chosen $\\textbf h$ we are maximizing\n",
    "\n",
    "$$ log p_{model}(\\textbf h, \\textbf x) = log p_{model}(\\textbf h) + log p_{model}(\\textbf x | \\textbf h), $$\n",
    "\n",
    "where the $log p_{model}(\\textbf h)$ term can be sparsity inducing with the use of an absolute value penalty such as the Laplace prior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than adding a penalty to the cost function, we can obtain an autoencoder that learns something useful by changing the reconstruction error term of the cost function. \n",
    "\n",
    "A standard autoencoder minimizes a loss function \n",
    "\n",
    "$$ L(\\textbf x, g(f(\\textbf x))), $$\n",
    "\n",
    "where L is a loss function penalizing the output for being different from the input. An example of this is an $L^2$ norm between $\\textbf x$ and $g(f(\\textbf x)).$ This encourages the model to form an identity function mapping $x$ to $g(f(\\textbf x)).$\n",
    "\n",
    "A **denoising autoencoder** instead minimized a locc function\n",
    "\n",
    "$$ L(\\textbf x, g(f(\\mathbf{\\tilde{x}}))),$$\n",
    "\n",
    "where $\\mathbf{\\tilde{x}}$ is a copy of x that has been corrupsted by some form of noise. A denoising autoencoder, then, is tasked with learning to undo this corruption rather than forming an identity map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing by Penalizing Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another regularization strategy for autoencoders is to penalize the derivatives of $h$ rather than its norm. Here, we sill have \n",
    "\n",
    "$$ L(\\textbf x, g(f(x))) + \\Omega (\\textbf h, \\textbf x), $$\n",
    "\n",
    "except now\n",
    "\n",
    "$$ \\Omega (\\textbf h, \\textbf x) = \\lambda \\sum_{i} || \\triangledown_{x} h_{i} ||^2.  $$\n",
    "\n",
    "This forces the autoencoder to find a model that is insensitive to slight changes in $textbf x$. This type of autoencoder is referred to as a **contractive autoencoder**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representational Power, Layer Size, and Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders do not have to be single-layer. In fact, they can benefit from depth in the same ways feedforward networks do. Depth can reduce both computational cost and the amount of training data needed to effectively represent the input distribution. A common strategy for training deep autoencoders is to greedily pretrain a stack of shallow autoencoders before training the full network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A denoising autoencoder is an autoencoder whose input has been corrupted. The goal of this model, then, is to learn to reconstruct the original input distribution given the corrupted unput $p(\\textbf x | \\mathbf{\\tilde{x}}).$ The corruption acts as a regularizer, preventing the model from learning a simple identity function. This model can be trained the same way as a standard autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Manifolds with Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders exploit the idea that data concentrates around a low-dimensional manifold or  a small set of such manifulds by attempting to learn the structure of this (these) manifold(s). \n",
    "\n",
    "An important characterization of a manifold is the set of its tangent planes. The tangent plane at point $\\textbf x$ on a $d$ dimensional manifold is given by $d$ basis vectors that span the local directions of variation allowed on the manifold. \n",
    "\n",
    "An autoencoder's training procedure represents a compromise between the following two forces:\n",
    "\n",
    "* learning a representation $\\textbf h$ of input $\\textbf x$ from the training distribution such that $\\textbf x$ can be approximately recovered from $\\textbf h$ through a decoder\n",
    "* Satisfying the regularization constraint, architectural or cost-function-based, making the model less sensitive to its input.\n",
    "\n",
    "Together these forces cause the model to learn a hidden representation that captures information about the data generating distribution. The important principle here is that the autoencoder can only afford to represent the variations that are needd in order to reconstruct the training examples. Without regularization, it will learn unnecessary details about the training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contractive Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contractive autoencoder, mentioned earlier, introduces a regularizer on the code $\\textbf h = f(\\textbf x),$ encouraging the derivatives of $f$ to be as small as possible: \n",
    "\n",
    "$$ \\Omega (\\textbf h) = \\lambda \\left | \\left | \\frac{\\partial f(\\textbf x)}{\\partial \\textbf x} \\right| \\right|_{F}^2. $$\n",
    "\n",
    "This penalty on $\\triangledown_{x} \\textbf h$ is the squared Frobenius norm (sum of squared elements) of the Jacobian of partial derivatives of the encoder function with respect to $\\textbf x$. The result of this penalty is a model that is insensitive to small changes in $\\textbf x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Sparse Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
