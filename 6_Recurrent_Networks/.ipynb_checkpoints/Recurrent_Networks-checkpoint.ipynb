{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Modeling: Recurrent and Recursive Nets\n",
    "**Goodfellow Chapter 10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent neural networks**, or RNNs, are a family of neural networks for processing sequential data. Similar to how a convolutional network is designed to process a grid of values $\\textbf X,$ an RNN is designed to process a sequence of values $\\textbf{x}^{1}, . . ., \\textbf{x}^\\tau$. RNNs can process longer sequences than would be practical with other architectures, and can typically process sequences of variable length.\n",
    "\n",
    "The key idea needed to move from feedforward to recurrent networks is parameter sharing. RNNs need to share parameters across different parts of the model in order to generalize to different sequence lengths. This is also important because the same piece of information can often appear in different positions within a sequence. \n",
    "\n",
    "Suppose we want to train a model to extract years from text. The sentence \"I went to Nepal in 2009\" contains the same information if it is rewritten as \"In 2009, I went to Nepal.\" A traditional feedforward network, trained on sentences of a fixed length with a different parameter for each input feature, would need to learn what a year looked like in each position of the sentence. This requires a lot of redundant learning. An RNN, alternatively, shares the same weights across several time steps, eliminating the need to re-learn the rules of language for each position in a sequence.\n",
    "\n",
    "An RNN shares parameters by passing a function of each step's output to the next position in the sequence. This way, the model always has a sense of context learned from previous steps' parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfolding Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a computational graph is a way to formalize the structure of a set of computations, mapping inputs and parameters to outputs and loss values. The feature of the computational graph that makes a network _recurrent_ is a connection between the nodes of one sequence's graph and another's. This requires **unfolding** the graph to represent  its multiple time steps to represent a chain of events. \n",
    "\n",
    "For example, consider the classical form of a dynamical system:\n",
    "\n",
    "$$ \\textbf{s}^{(t)} = f(\\textbf{s}^{(t-1)};\\mathbf{\\theta}), $$\n",
    "\n",
    "where $\\textbf{s}^{(t)}$ is called the state of the system. This equation is recurrent because at each step, it calls upon the same function at a previous state.\n",
    "\n",
    "For a finite number of steps $\\tau,$ we can unfold this graph by applying the function $\\tau - 1$ times. For example, with $\\tau = 3,$ the unfolded version of the above graph becomes:\n",
    "\n",
    "\n",
    "$$\n",
    "\\textbf{s}^{(3)} = f(\\textbf{s}^{(2)};\\mathbf{\\theta}) \\\\\n",
    "= f(f(\\textbf{s}^{(1)};\\mathbf{\\theta});\\mathbf{\\theta})\n",
    "$$\n",
    "\n",
    "The unfolded function is no longer recurrent, and can now be represented as an acyclic graph.\n",
    "\n",
    "A typical RNN will use the following equation, differing only in its use of the input data $\\mathbf{x}^{(t)},$ with $\\textbf{h}$ representing the model state:\n",
    "\n",
    "$$ \\textbf{h}^{(t)} = f(\\textbf{h}^{t - 1}, \\textbf{x}^{t};\\mathbf{\\theta}) $$\n",
    "\n",
    "When an RNN is tasked with predicting the future given the past items in a sequence, it learns to use $\\textbf{h}^{(t)}$ as a lossy summary of the task-relevant aspects of the past sequence of inputs up to $t.$ This summary is in general lossy, since it maps an arbitrary length sequence to a fixed length vector $\\textbf{h}^{(t)}.$ This will typically involve the state \"forgetting\" pieces of the past sequence that it has deemed irrelevant. \n",
    "\n",
    "The unfolded recurrence after $t$ steps can be represented as:\n",
    "\n",
    "$$ \n",
    "\\textbf{h}^{(t)} = g^{(t)}(x^{(t)}, x^{(t-1)}, . . ., x^{(2)}, x^{(1)})) \\\\\n",
    "= f(\\textbf{h}^{(t-1)}, \\textbf{x}^{(t)};\\mathbf{\\theta}).$$\n",
    "\n",
    "The advantages of this unfolded representation are:\n",
    "\n",
    "* The learned model always has the same input size regardless of sequence length\n",
    "* It is possible to use the same transition function $f$ with the same parameters at every step\n",
    "\n",
    "This means we get to learn a single model $f$ that operates on all times steps and all sequence lengths, rather than needing a separate model $g^{(t)}$ for each possible time step. \n",
    "\n",
    "Learning a single shared model allows generalization to sequence lengths that did not appear in the training set, and enables the model to be estimated with relatively few training examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few design patterns for RNNs are:\n",
    "\n",
    "* Producing an output at each time step, with recurrent connections between hidden units\n",
    "* Producing an output at each time step, with a recurrent connection between the output on step t-1 and the hidden unit of step t\n",
    "* RNNs with recurrent connections between hidden units, producing a single output after processing the entire sequence\n",
    "\n",
    "A standard RNN forward propogation would be:\n",
    "\n",
    "$$ \\textbf{a}^{(t)} = \\textbf b + \\textbf W \\textbf{h}^{t-1} + \\textbf{Ux}^{(t)}, $$\n",
    "\n",
    "$$ \\textbf{h}^{(t)} = \\text{tanh} \\left ( \\textbf{a}^{(t)} \\right ), $$\n",
    "\n",
    "$$ \\textbf{o}^{(t)} = \\textbf c + \\textbf{Vh}^{(t)}, $$\n",
    "\n",
    "$$ \\mathbf{\\hat{y}}^{(t)} = \\text{softmax} \\left ( \\textbf{o}^{(t)} \\right ),$$\n",
    "\n",
    "where $\\textbf U , \\textbf V, \\text{and } \\textbf W$ are the input-to-hidden, hidden-to-output, and recurrent hidden-to-hidden weights respectively. The loss, then, would be the sum of the log losses for each time step. Backpropogation through an RNN involves moving back through each time step, end to beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Forcing and Networks with Output Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks with output recurrence are strictly less powerful than nestworks with hidden layer recurrence. Models with this type of recurrence can be trained with **teacher forcing**. This method simply involves feeding the previous step's output as an input to a hidden layer in the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Gradient in an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[yikes, come back to this]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNNs discussed so far take into accound only past and present information from the sequence (x_1, ..., x_t). Many sequence processing tasks, however, require us to take into account information from both before and after the time-step of interest. Take voice recognition as an example. In order to guess the words being spoken in raw audio, we need to know the sounds happening both before and after the sounds at step t in the sequence. This is because transition probabilities between sounds (i.e. the likelihood of certain phonemes happening before and after a candidate word) carry crucial information for this task. For this reason, it would be useful to have a recurrent model that is able to look into the future for added context. \n",
    "\n",
    "Bidirecitonal RNNs answer this need, combining an RNN that moves forward through time, beginning at the sequence start, and a second RNN that moves backward, starting at the end of the sequence and moving toward position 0. These RNNs have two states for the forward and backward-moving networks, which effectively represent knowledge of the past and future of the sequence at each time step. Similar representations can be formed for two-dimensional data such as images, with four RNNs, one moving in each direction (up/down/left/right).\n",
    "\n",
    "Bidirectional RNNs have shown success in speech, handwriting recognition, and bioinformatics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Sequence-to-Sequence Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often want our RNN to be able to map variable-length input sequences to variable-length output sequences. Examples of this include translation and question-answering tasks. The simplest architecture for doing this is called either an **encoder-decoder** or **sequence-to-sequence** model.\n",
    "\n",
    "The approach is:\n",
    "\n",
    "* First, an **encoder** or **reader** or **input** RNN processes the input sequence. The encoder emits a contex C, usually a simple function of its final hidden state\n",
    "* Then, a **decoder** or **writer** or **output** RNN is conditioned on the fixed-length vector passed by the previous RNN to generate the output sequence $P(Y=(y^{(1)}, ..., y^{(n_{y})}).$\n",
    "\n",
    "The benefit of such a model is that the two lengths, $n_{y} \\text{ and } n_{x}$ do not need to be the same, where other architectures have the constraint $n_{y} = n_{x} = \\tau.$ \n",
    "\n",
    "The two RNNs are trained jointly to maximize the average of $\\text{log}P(y^{(1)}, ..., y^{(n_{y})}|x^{(1)}, ..., x^{(n_{x})})$ over all (x,y) sequences. The final state $\\mathbf{h}_{n_{x}}$ of the encoder is typically used as a representation C of the input sequence that is provided as input to the decoder RNN. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge of Long-Term Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Echo State Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky Units and Other Strategies for Multiple Time Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Skip Connections through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Units and a Spectrum of Different Time Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Long Short-Term Memory (LSTM) and Other Gated RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Gated RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization for Long-Term Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing to Encourage Information Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
