{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics and Probability Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Terminology\n",
    "\n",
    "* **Scalars**: single numbers. 5, 3.14, and 0.05 are all scalars. These are typically given lowercase variable names.\n",
    "* **Vectors**: a vector is an array of numbers, arranged in order. These numbers are identified by their place in this ordering. Vectors are usually given lowercase variable names, which are written in bold. e.g.: \n",
    "\n",
    "$$     \\boldsymbol{x}=\\begin{bmatrix}\n",
    "         X_1 \\newline\n",
    "         X_2 \\newline\n",
    "         X_3 \\newline\n",
    "         X_4\n",
    "        \\end{bmatrix} $$\n",
    "        \n",
    "* **Matrices**: a matrix is a 2-D array of numbers, with each element being identifies by two indices. These are given uppercase names, written in bold. $\\boldsymbol{A}$ denotes matrix A, and $A_{i,j}$ denotes the element in row $i$, column $j$. \n",
    "* **Tensors**: tensors are arrays with more than two axes. Tensors, similar to arrays, are given bold capitalized names and referred to by their indexes $(i,j,k)$.\n",
    "* **Shape**: a vector, matrix, or tensor's shape is simply its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#examples in NumPy\n",
    "import numpy as np\n",
    "x = np.array([1,2,3]) #a vector. (3 x 1)\n",
    "A = np.matrix([[1,2,3],\n",
    "             [4,5,6],\n",
    "             [7,8,9]]) #a matrix. (3 x 3)\n",
    "B = np.array([A, A, A]) #an array of matrices. (3 x 3 x 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposes\n",
    "\n",
    "A simple but important matrix operation is a **transpose**. To take the transpose of a matrix, swap its rows with its columns. The first row becomes the first column, the second row becomes the second column, and so on. In the case of a vector, taking its transpose turns it from a row vector into a column vector, or visa-versa. For example, \n",
    "$$\n",
    "\\textbf{A} = \\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    4 & 5 & 6 \n",
    "  \\end{bmatrix} \\text{ and  }\n",
    "\\textbf{A}^\\intercal = \\begin{bmatrix}\n",
    "    1 & 4 \\\\\n",
    "    2 & 5 \\\\\n",
    "    3 & 6\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "\n",
      "[[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]]\n"
     ]
    }
   ],
   "source": [
    "# a transpose in numpy is taken by adding '.T' to a matrix or vector\n",
    "print A\n",
    "print '\\n'\n",
    "print A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Addition\n",
    "Matrices and vectors can each be added or subtracted from one-another, as long as they are of the same shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2,  4,  6],\n",
       "        [ 8, 10, 12],\n",
       "        [14, 16, 18]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "Two matrices be multiplied if an only if the number of columns in the first matrix matches the number of rows in the second. The resultt of multipling (m x n) matrix A by (n x p) matrix B is a third matrix C with dimensions (m x p). Note that vectors can also be multiplied with matrices under this same rule, since a vector of length m is also a (m x 1) matrix.\n",
    "\n",
    "A few properties to keep in mind:\n",
    "* Matrix multiplication is distributive. $\\textbf{A}(\\textbf{B} + \\textbf{C}) = \\textbf{AB} + \\textbf{BC}$\n",
    "* It is also associative. $\\textbf{A}(\\textbf{BC}) = (\\textbf{AB})\\textbf{C}$\n",
    "* Matrix multiplication is not commutative. $\\textbf{AB} \\neq \\textbf{BA}$\n",
    "* $\\textbf{AB}^\\intercal = \\textbf{A}^\\intercal \\textbf{B}^\\intercal$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 30,  36,  42],\n",
       "        [ 66,  81,  96],\n",
       "        [102, 126, 150]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in numpy:\n",
    "np.matmul(A,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity and Inverse Matrices\n",
    "\n",
    "The **identity matrix** is a matrix that does not change a vector when the two are multiplied. It is a square matrix with $n$ elements where the values along its diagonal are 1, and each other value is 0. The notation for the identity matrix is $\\textbf{I}_{n}$. \n",
    "\n",
    "The **inverse matrix** of $\\textbf{A}$ is denoted $\\textbf{A}^{-1}$, and is defined such that \n",
    "$$ \\textbf{A}^{-1} \\textbf{A} = \\textbf{I}_{n}. $$\n",
    "\n",
    "It is important to note that $\\textbf{A}^{-1}$ does not always exist. For the conditions under which $\\textbf{A}^{-1}$ exists, see the [Invertible Matrix Theorem](http://mathworld.wolfram.com/InvertibleMatrixTheorem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-24.  18.   5.]\n",
      " [ 20. -15.  -4.]\n",
      " [ -5.   4.   1.]]\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "[[  1.00000000e+00  -3.55271368e-15  -1.42108547e-14]\n",
      " [ -2.66453526e-15   1.00000000e+00   0.00000000e+00]\n",
      " [  8.88178420e-16   0.00000000e+00   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "A = np.matrix([[1,2,3],[0,1,4],[5,6,0]])\n",
    "print np.linalg.inv(A)\n",
    "print np.identity(3)\n",
    "print np.matmul(np.linalg.inv(A), A) #close, but not exactly I due to approximation errors made by the computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Dependence and Span\n",
    "\n",
    "The **span** of a set of vectors is the set of all points obtainable by linear combinations of the set of vectors, defined as:\n",
    "$$ span(\\textbf{v}) = \\sum_{i} c_{i} \\textbf{v}^{(i)} \\;|\\;{c_i} \\in \\mathbb{R}  $$\n",
    "\n",
    "Determining whether $\\textbf{A} \\textbf{x} = \\textbf{b}$ has a solution is therefore a test of whether $\\textbf{b}$ is in the span of the columns of $\\textbf{A}$. \n",
    "\n",
    "Similarly, if any vector in matrix $\\textbf{A}$ is a linear combination of the others, then the set of vectors is **linearly dependent**. Otherwise, if there is no solution to this problem, then they are **linearly independent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms\n",
    "\n",
    "A **norm** is a function used for measuring the size of vectors. The $L^p$ nofm is defined as\n",
    "$${\\displaystyle \\left\\|x\\right\\|_{p}= (\\sum_{i}\\left(|x_{i}|^{p})\\right)^{\\frac {1}{p}}.} $$\n",
    "\n",
    "for $p \\in \\mathbb{R}, p \\geq 1$.\n",
    "\n",
    "Norms map vectors to non-negative values, essentially measuring their distance from the origin. The $L^2$ norm is simply a vector's Euclidean distance from the origin. The $L^1$ norm is also commonly used, using absolute values instead of squared terms, defined as\n",
    "$$ {\\displaystyle \\left\\|x\\right\\|_{1}=  \\sum_{i}|x_{i}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "5.74456264654\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2,5,-2])\n",
    "print np.linalg.norm(x, 1) #l1 norm\n",
    "print np.linalg.norm(x, 2) #l2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Matrices\n",
    "A **diagonal** matrix has all zeros except for along the diagonal. Unlike the identity matrix, however, the diagonal values do not have to equal one. These are denoted $diag(\\textbf{v})$, where $\\textbf{v}$ is the vector of values that go across the diagonal of the square matrix $diag(\\textbf{v})$.\n",
    "\n",
    "A **symmetric** matrix is a matrix that is equal to its own transpose, so that $\\textbf{A} = \\textbf{A}^T$\n",
    "\n",
    "An **orthogonal** matrix is a square matrhix whose rows are mutually orthonormal and whose columns are also mutually orthonormal:\n",
    "\n",
    "$$ \\textbf{A}^T \\textbf{A} = \\textbf{A} \\textbf{A}^T = \\textbf{I}. $$\n",
    "\n",
    "This implies that \n",
    "$$ \\textbf{A}^{-1} = \\textbf{A}^T. $$\n",
    "\n",
    "Orthogonal matrices are therefore of interest because their inverse is computationaly inexpensive to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "\n",
    "Eigendecomposition is the process of decomposing a matrix into its **eigenvectors** and **eigenvalues**. \n",
    "\n",
    "An eigenvector of a square matrix $\\textbf{A}$ is a nonzero vector $\\textbf{v}$ such that multiplication by $\\textbf{A}$ alters only the scale of $\\textbf{v}$:\n",
    "\n",
    "$$ \\textbf{A}\\textbf{v} = \\lambda\\textbf{v}.$$\n",
    "\n",
    "The scalar value $\\lambda$ is known as the **eigenvalue** corresponding to this **eigenvector**. Suppose matrix $\\textbf{A}$ has $n$ linearly independent eigen vectors {${v_{1} v_{2} . . . v_{n}}$} with corresponding eigenvalues {${\\lambda_{1} \\lambda_{2} . . . \\lambda_{n}}$}. We can then concatenate the eigenvectors to form a matrix $\\textbf{V}$, and do the same with the eigenvalues to form vector $\\lambda$. The **eigendecomposition** of $\\textbf{A}$, then, is written:\n",
    "\n",
    "$$\\textbf{A} = \\textbf{V}\\text{diag}(\\lambda)\\textbf{V}^{-1}.$$\n",
    "\n",
    "In most cases we will encounter in machine learning, the decomposition will be simple, in that it will not involve complex eigenvalues:\n",
    "\n",
    "$$\\textbf{B} = \\textbf{Q}\\textbf{A}\\textbf{Q}^{-1},$$\n",
    "\n",
    "where $\\textbf{Q}$ is an orthogonal matrix composed of the eigenvectors of $\\textbf{B}$, and $\\textbf{A}$ is the diagonal matrix. \n",
    "\n",
    "A few additional notes on eigendecomposition:\n",
    "* A matrix whose eigenvalues are all positive is called **positive definite**\n",
    "* A matrix whose eigenvalues are all $\\geq 0$ is called **positive semidefinite**\n",
    "* A matrix is singular if and only if one or more of its eigenvalues are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.2296696 , -0.02635282,  7.25602242]),\n",
       " matrix([[ 0.22578016, -0.75769839, -0.49927017],\n",
       "         [ 0.52634845,  0.63212771, -0.46674201],\n",
       "         [-0.81974424, -0.16219652, -0.72998712]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.linalg.eigh() returns the eigenvalues and eigenvectors of a square matrix\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "Eigendecomposition decomposes a matrix into eigenvectors and eigenvalues. **Singular Value Decomposition (SVD)** provides another way to factorize a matrix into **singular vectors** and **singular values**. The value of SVD over eigendecomposition is that it is more generalizeable. SVD will work on a square matrix, for example, while eigendecomposition will not. \n",
    "\n",
    "Where eigendecomposition rewrites matrix $\\textbf{A}$ using a matrix $\\textbf{V}$ and vector of eigenvalues $\\lambda$ as:\n",
    "\n",
    "$$ \\textbf{A} = \\textbf{V}\\text{diag}(\\lambda)\\textbf{V}^{-1}, $$\n",
    "\n",
    "SVD rewrites A using three matrices:\n",
    "\n",
    "$$ \\textbf{A} = \\textbf{U}\\textbf{D}\\textbf{V}^T, $$\n",
    "\n",
    "where, if $\\textbf{A}$ is $(m \\times n)$, then $\\textbf{U}$ is $(m \\times m)$, $\\textbf{D}$ is $(m \\times n)$, and $\\textbf{V}$ os $(n \\times n).$ $\\textbf{U} \\text{and} \\textbf{V}$ are orthogonal, and $\\textbf{V}$ is diagonal, but not necessarily square. The elements along the diagonal of $\\textbf{D}$ are known as **singular values** of $\\textbf{A}$.\n",
    "\n",
    "SVD is also necessary for partially generalizing matrix inversion to nonsquare matrices (see the next section on pseudo inverses.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[-0.33780332, -0.51318841, -0.78900353],\n",
       "         [-0.19885965, -0.78044262,  0.59275978],\n",
       "         [-0.91996943,  0.35713718,  0.16158364]]),\n",
       " array([ 8.27883923,  4.84357297,  0.02493818]),\n",
       " matrix([[-0.59641821, -0.77236466, -0.2184906 ],\n",
       "         [ 0.26271876,  0.06937103, -0.96237545],\n",
       "         [ 0.75846171, -0.63137983,  0.16154054]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Moore-Penrose Pseudoinverse\n",
    "\n",
    "Matrix inversion is not defined for non-square matrices. The **Moore-Penrose Pseudoinverse** is a partial workaround to this. The pseudo inverse of A is defined as:\n",
    "\n",
    "$$ \\textbf{A}^{+}=\\lim _{\\delta \\searrow 0}(\\textbf{A}^{T}\\textbf{A}+\\delta \\textbf{I})^{-1}\\textbf{A}^T. $$\n",
    "\n",
    "This value is computed using the formula $\\textbf{A}^{+}= \\textbf{V}\\textbf{D}^{+}\\textbf{U}^T$, where $\\textbf{U}$, $\\textbf{V}$, and $\\textbf{D}$ are the SVD of $\\textbf{A}$, and the pseudoinverse of $\\textbf{D}$ is obtained by taking the reciprocal of its nonzero elements and the taking the transpose of the resulting matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-24.,  18.,   5.],\n",
       "        [ 20., -15.,  -4.],\n",
       "        [ -5.,   4.,   1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants\n",
    "\n",
    "The determinant of a square matrix, denoted $\\text{det}(\\textbf{A})$, maps the matrix to a real scalar. This value is equal to the product of the eigenvales of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999867"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
